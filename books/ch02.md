# ch.2 맵리듀스
MapReduce는 대용량 데이터를 분산 환경에서 병렬 처리하기 위해 설계된 프로그래밍 모델이자 처리 프레임워크를 의미한다.
## 기상 데이터셋
기상 데이터셋을 활용하여 효율적으로 기온 정보를 추출하는 맵리듀스 활용사례를 살펴본다

### 데이터셋 구조
```
0057
332130 # USAF weather station identifier
99999 # WBAN weather station identifier
19500101 # observation date
0300 # observation time
4
+51317 # latitude (degrees x 1000)
+028783 # longitude (degrees x 1000)
FM-12
+0171 # elevation (meters)
99999
V020
320 # wind direction (degrees)
1 # quality code
N
0072
1
00450 # sky ceiling height (meters)
1 # quality code
C
N
010000 # visibility distance (meters)
1 # quality code
N
9
-0128 # air temperature (degrees Celsius x 10)
1 # quality code
-0139 # dew point temperature (degrees Celsius x 10)
1 # quality code
10268 # atmospheric pres
```
특정 관측소에서 특정 시점에 측정한 기상데이터를 다음과 같이 저장한다. 위와 같은 정보가 한줄 씩 한 파일에 기록되어 있다. 즉 한 파일에는 여러 줄이 있고 각 줄은 특정 관측소의 특정 시점의 데이터를 가지고 있다. 

```
% ls raw/1990 | head
010010-99999-1990.gz
010014-99999-1990.gz
010015-99999-1990.gz
010016-99999-1990.gz
010017-99999-1990.gz
010030-99999-1990.gz
010040-99999-1990.gz
010080-99999-1990.gz
010100-99999-1990.gz
010150-99999-1990.gz
```
위와 같이 연도 디렉토리에 가면 다음과 같은 파일들이 gz으로 압축되어 저장된다. 파일명을 분석해보면 010010-99999 관측소의 1990년 데이터를 의미한다.  

이 데이터셋을 활용하여 연도별 최고 기온을 계산하기 위한 다양한 방법을 알아본다.

## 유닉스 도구로 데이터 분석하기

### awk
awk는 텍스트 파일을 행 단위로 처리하며, 각 행을 패턴 매칭과 필드 기반으로 분석하거나 가공하는 데 사용하는 강력한 텍스트 처리 언어이다. 
```
awk 'pattern { action }' file
```
- pattern은 조건 (예: 특정 문자열 포함 여부)
- action은 패턴이 맞았을 때 실행할 코드 블록 (예: 출력, 계산 등)
- file은 처리할 텍스트 파일

### 스크립트
```
#!/usr/bin/env bash

# 'all/' 디렉토리 안의 모든 .gz 파일에 대해 반복
for year in all/*
do
  # 파일 이름에서 디렉토리 경로와 .gz 확장자 제거
  echo -ne "$(basename "$year" .gz)\t"

  # 압축 해제한 내용을 AWK로 처리하여 최대 기온을 출력
  gunzip -c "$year" | \
  awk '
    {
      temp = substr($0, 88, 5) + 0       # 온도 추출 (정수 변환)
      q = substr($0, 93, 1)              # 품질 코드 추출
      if (temp != 9999 && q ~ /[01459]/ && temp > max)
        max = temp                       # 최대값 갱신
    }
    END {
      print max                          # 최종 최대 기온 출력
    }
  '
done
```

위 스크립트는 다음과 같이 동작한다.
1. basename "$year" .gz → 파일 이름에서 확장자 .gz 제거
2. gunzip -c "$year" → 압축 해제한 내용을 표준 출력으로 전달
3. substr($0, 88, 5) → 한 줄에서 88~92번째 위치의 문자열 (기온)
4. substr($0, 93, 1) → 93번째 위치의 문자열 (기온 품질 코드)
5. q ~ /[01459]/ → 기온 품질이 "신뢰할 수 있음"일 때만 통과
6. max → 현재까지의 최대값 저장

### 문제점 
- 위 스크립트는 하나의 프로세스로 동작하여 수행시간이 오래걸린다.
- 연도별로 별도의 스크립트를 작성하여 병렬프로세스로 수행시킨다고 하더라도 다음과 같은 단점이 추가로 존재한다
  - 일을 동일한 크기로 나누어 프로세스에 할당하기에 모호하다. 연도별로 파일크기가 다르기 때문에 결국 가장 큰 용량의 연도를 담당하는 프로세스의 작업이 끝날 때까지 전체 작업은 마무리 될 수 없고 다른 프로세스는 쉬게 된다. 
  - 대안으로는 전체 입력 파일을 특정 크기의 청크로 나누어 각 청크를 프로세스에 할당하는 방식을 사용할 수 있다. 하지만 청크로 나누어 관측소별 최고 기온을 구하더라도 이를 결국에는 하나로 합쳐서 최댓값을 구해야하는 연산이 수행되어야 한다. 
  - 아무리 병렬 처리를 하더라도 단일 머신의 성능에는 한계가 있다. 

## 하둡으로 데이터 분석하기
### 맵과 리듀스
```
0067011990999991950051507004...9999999N9+00001+99999999999...
0043011990999991950051512004...9999999N9+00221+99999999999...
0043011990999991950051518004...9999999N9-00111+99999999999...
0043012650999991949032412004...0500001N9+01111+99999999999...
0043012650999991949032418004...0500001N9+00781+99999999999...
```
실제 작업을 할 때 input 데이터는 위와 같다. 

```
(0, 0067011990999991950051507004...9999999N9+00001+99999999999...)
(106, 0043011990999991950051512004...9999999N9+00221+99999999999...)
(212, 0043011990999991950051518004...9999999N9-00111+99999999999...)
(318, 0043012650999991949032412004...0500001N9+01111+99999999999...)
(424, 0043012650999991949032418004...0500001N9+00781+99999999999...)
```
map reduce는 데이터를 입력할 때 (key, value) 형태로 입력을 받는데, 이 데이터는  key value file내 offset을 key로 각 줄의 데이터를 value로 설정하면 된다. 


```
(1950, 0)
(1950, 22)
(1950, −11)
(1949, 111)
(1949, 78)
```
이 데이터를 기반으로 맵 작업을 통해 각 줄의 연도와 온도정보를 반환 받을 수 있다. 

```
(1949, [111, 78])
(1950, [0, 22, −11])
```
맵의 출력결과는 다시 리듀스의 입력으로 들어가는데 이 작업도 동일하게 key, value 형태로 제공해야하고 다음과 같이 리듀스의 입력을 설정할 수 있다.

```
(1949, 111)
(1950, 22)
```
리듀스 작업을 마치면 다음과 같이 우리가 원하던 연도별 최고 기온을 구할 수 있다. 

![](https://velog.velcdn.com/images/fadfa/post/e351a302-5015-42a8-8f0e-9b20af6c8176/image.png)
위 작업을 정리하면 다음과 같다. 
## 자바 맵리듀스
### Mapper
```
import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MaxTemperatureMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private static final int MISSING = 9999;

    @Override
    public void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String year = line.substring(15, 19);
        int airTemperature;

        // parseInt doesn't like leading plus signs
        if (line.charAt(87) == '+') {
            airTemperature = Integer.parseInt(line.substring(88, 92));
        } else {
            airTemperature = Integer.parseInt(line.substring(87, 92));
        }

        String quality = line.substring(92, 93);

        if (airTemperature != MISSING && quality.matches("[01459]")) {
            context.write(new Text(year), new IntWritable(airTemperature));
        }
    }
}

```
Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>를 통해 타입을 미리 지정한다. 참고로 hadoop은 hadoop과 java간 적절한 형변환을 위해 IntWritable, LogWritable같은 클래스를 제공한다. 

- 입력 키 (KEYIN): LongWritable → 파일에서 줄의 위치 (offset), 무시함
- 입력 값 (VALUEIN): Text → 한 줄 텍스트 전체
- 출력 키 (KEYOUT): Text → 연도
- 출력 값 (VALUEOUT): IntWritable → 기온

Context에 map 의 출력을 담는 방식이다. 

### Reducer
```
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class MaxTemperatureReducer
        extends Reducer<Text, IntWritable, Text, IntWritable> {

    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int maxValue = Integer.MIN_VALUE;

        for (IntWritable value : values) {
            maxValue = Math.max(maxValue, value.get());
        }

        context.write(key, new IntWritable(maxValue));
    }
}
```
맵퍼가
```
("1950", 0)
("1950", 22)
("1950", -11)
("1949", 111)
("1949", 78)
```
위와 같은 데이터를 넘겨주면 

```
("1950", 22)
("1949", 111)
```
위와 같이 출력을 하는 역할을 한다. 

### 수행 애플리케이션
```
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class MaxTemperature {

    public static void main(String[] args) throws Exception {
        // 입력 인자가 2개가 아니면 에러 출력 후 종료
        if (args.length != 2) {
            System.err.println("Usage: MaxTemperature <input path> <output path>");
            System.exit(-1);
        }

        // 새로운 Hadoop 작업(Job) 생성
        Job job = new Job();
        job.setJarByClass(MaxTemperature.class);  // 실행할 클래스 (jar 내부 메인 클래스 설정)
        job.setJobName("Max temperature");         // Job 이름 설정

        // 입력 경로와 출력 경로 설정
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        // 사용할 Mapper 및 Reducer 클래스 지정
        job.setMapperClass(MaxTemperatureMapper.class);
        job.setReducerClass(MaxTemperatureReducer.class);

        // 출력 데이터의 키와 값 타입 지정
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 작업 실행 및 결과에 따라 0(성공) 또는 1(실패)로 종료
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

해당 애플리케이션을 수행하면 모든 데이터노드에 해당 job을 전달하여 수행할 수 있게 한다. 

## 분산형으로 확장하기
- Job: 클라이언트가 수행하는 작업의 기본 단위
- Task: Job을 여러개의 작업으로 나누어 처리하는데 이 때의 작업 단위를 Task라고 한다.

하둡은 Job을 input split또는 split으로 나누어 고정 크기 조각으로 분리하고 각 split을 여러 서버에 분산시켜 작업하게 한다. 적절한 split의 크기는 hdfs의 블록사이즈이다. 

task를 각 노드에 분배할 때 data locality의 이점을 활용할 수 있도록 분배한다.
![](https://velog.velcdn.com/images/fadfa/post/16cdda5e-10ac-448c-968f-7dcdceedd08c/image.png)
- task의 데이터가 같은 노드에 존재하는 경우 (a)
- task의 데이터가 다른 노드에 있지만 같은 랙에 존재하는 경우 (b)
- task의 데이터가 아예 다른 랙에 존재하는 경우 (c)
리듀스 태스크는 어차피 네트워크를 타는 작업이므로 map 작업만큼 네트워크 cost를 크게 생각하지 않아도 된다. 

### 분산 리듀스 작업
![](https://velog.velcdn.com/images/fadfa/post/e632e332-c292-40f9-9b1c-40568c9a0f00/image.png)
리듀스 태스크는 위와 같이 key를 적절히 hashing하여 분산 리듀스 작업을 할 수 있다. 
리듀스 수를 선택하는 것은 job의 실행 시간에 미치는 영향이 매우 크므로 튜닝이 필요하다. 

### 리듀스가 없는 맵리듀스
![](https://velog.velcdn.com/images/fadfa/post/0ba79cca-07a8-450a-b0a7-b68223793a9b/image.png)
리듀스가 없는 맵리듀스 작업도 있고 이는 다음과 같이 수행된다. 대표적인 예로 distcp가 있다. 

### 컴바이너 함수
combiner는 map의 결과를 reduce로 보내기 전에 중간 데이터를 로컬에서 미리 줄여주는 함수이다. 
```
Mapper1: (1950, 0), (1950, 20), (1950, 10)
Mapper2: (1950, 25), (1950, 15)

Reducer: (1950, [0, 20, 10, 25, 15])
```
맵출력이 위와 같다고 한다면 reducer에는 다음이 들어간다

```
Mapper1: (1950, 0), (1950, 20), (1950, 10)
Mapper2: (1950, 25), (1950, 15)

Combiner1: (1950, 20)
Combiner2: (1950, 25)

Reducer: (1950, [20, 25])
```
Combiner를 사용하면 Reducer의 입력이 줄어드므로 성능상에 이점을 가져갈 수 있다. 

## 하둡 스트리밍
Hadoop Streaming은 MapReduce 작업을 Java가 아닌 스크립트 언어 (Python, Ruby, etc.)로 작성할 수 있도록 해주는 Hadoop의 기능입니다.
- 입력은 표준 입력(stdin)으로 넘기고
- 출력은 표준 출력(stdout)으로 받습니다


