- ch10 하둡 클러스터 설정
    
    안정적인 플랫폼을 위한 물리적, 논리적 기반을 다지는 과정 설명
    
    ## 1 클러스터 아키텍처 설계
    
    ### 1.1 하드웨어 사양 결정
    
    하둡 클러스터의 하드웨어 구성은 역할군에 따라 명확히 구분된다.
    
    마스터 노드(네임노드, 리소스매니저)는 클러스터의 두뇌역할
    
    워커 노드(데이터 노드, 노드 매니저)는 실제 데이터 저장과 연산 담당
    
    마스터 노드에 높은 ram과 적당한 cpu 
    
    → 네임노드는 전체 네임스페이스의 파일 및 블록에 대한 메타데이터를 모두 **메모리에** 보관해야한다.
    
    - 네임노드는 얼마나 많은 메모리를 필요로하나?
        
        각 파일의 개별 블록에 대한 참조 값은 모두 메모리상에서 관리되기 때문에 네임노드는 충분한 메모리가 필요하다.
        
        메모리 사용량은 파일당 블록 수, 파일명 길이, 파일시스템의 디렉터리 수에 의해 결정되기 때문에 정확한 공식을 제시하기는 어렵다.
        
        기본 1000MB 네임노드 메모리는 일반적으로 수백만개의 파일을 다룰때는 충분하다.
        
        어림잡아 계산해보면 저장소 백만 블록당 1000MB가 필요하다.
        
        ex)
        
        노드당 24TB 디스크로 구성된 200-노드 클러스터에서 
        
        블록사이즈는 128MB이고 복제 수준이 3이라면 대략 1250만개의 블록을 수용할 수 있다.
        
        **(200*24,000,000MB / (128MB*3)) ~= 1250만**
        
        이경우 네임노드 메모리의 초기 설정값으로는 12,000MB가 적절하다. (by 백만 블록당 1000MB)
        
    
    워커 노드에는 다수의 디스크와 충분한 cpu 코어 권장
    
    워커 노드의 디스크 구성으로 JBOD(just a bunch of disks) 강력히 권장
    
    저렴한 범용 하드웨어 여러대 묶어 소프트웨어 수준에서 장애를 극복
    
    RAID와 같은 하드웨어 수준 이중화는 디스크 장애를 컨트롤러가 숨겨주지만, **비용이 높고 특정 컨트롤러에 종속될 수 있음 (-)**
    
    하지만 HDFS는 **블록 복제라는 자체적인 데이터 안정성 메커니즘** 가진다.
    
    이에 RAID 컨트롤러 없이 디스크를 그대로 노출하는 JBOD 구성이 더 효율적이고 경제적이다.
    
    - **raid란? (GPT)**
        
        Hadoop 책에서 RAID와 JBOD 이야기가 나오는 이유는, **데이터 안정성을 어디서 어떻게 보장할 것인가**의 차이 때문이에요. 먼저 RAID 자체를 정리해드릴게요.
        
        ---
        
        # 🖥 RAID(Redundant Array of Independent/Inexpensive Disks)
        
        **여러 개의 디스크를 묶어서 하나의 논리적 디스크처럼 사용하는 기술**이에요.
        
        원래 목적은 **성능 향상**과 **데이터 안정성(장애 대비)** 두 가지를 동시에 달성하는 것.
        
        ---
        
        ## 🔑 RAID의 주요 레벨
        
        1. **RAID 0 (스트라이핑, Striping)**
            - 여러 디스크에 데이터를 **분산 기록** (예: 짝수 블록은 디스크1, 홀수 블록은 디스크2)
            - 장점: 성능↑ (병렬 I/O 가능)
            - 단점: **안정성 없음** → **한 디스크라도 망가지면 전체 데이터 손실**
            - 용도: 속도가 중요한 경우 (캐시, 임시 데이터 등)
        2. **RAID 1 (미러링, Mirroring)**
            - 같은 데이터를 두 개(또는 그 이상) 디스크에 **그대로 복제**
            - 장점: 안정성↑ (한 디스크 망가져도 문제 없음)
            - 단점: **저장 효율 ↓** (2TB + 2TB 디스크 = 2TB만 사용 가능)
        3. **RAID 5 (스트라이핑 + 패리티, Parity)**
            - 데이터를 여러 디스크에 분산 저장하면서, ****패리티 정보(오류 복구용 비트)**를 추가로 기록**
            - 장점: 안정성 + 저장 효율 균형 (n개의 디스크 중 n-1 용량 사용 가능)
            - 단점: 쓰기 성능 느림 (패리티 계산 때문에)
        4. **RAID 6**
            - RAID 5와 유사하지만, **이중 패리티** → **동시에 2개 디스크까지 장애 허용**
            - 안정성↑, 하지만 쓰기 더 느림
        5. **RAID 10 (RAID 1 + 0)**
            - RAID 1(미러링) + RAID 0(스트라이핑) 결합
            - 안정성과 성능을 모두 잡음
            - 단점: 디스크 많이 필요 (비용↑)
        
        ---
        
        ## 📌 RAID vs JBOD
        
        - **RAID**
            - **하드웨어/펌웨어 수준에서 디스크 장애를 “숨겨줌”** (사용자 입장에서는 디스크 장애 감지 어려움)
            - **컨트롤러 의존적** → 특정 벤더 장비 필요 → 비용 ↑
            - 하드웨어적으로 안정성 보장
        - **JBOD (Just a Bunch Of Disks)**
            - RAID 없이 **각 디스크를 독립적으로 운영**
            - **Hadoop(HDFS) 같은 분산 파일 시스템이** **소프트웨어적으로 안정성 보장 (블록 복제)**
            - 저렴하고 단순 → 대규모 분산 환경에서 경제적
        
        ---
        
        ## 🚩 Hadoop에서 JBOD 권장 이유
        
        - **HDFS 자체가 **3중 블록 복제(기본값)**로 데이터 안정성을 제공**
        - RAID 컨트롤러 같은 고가 하드웨어 불필요
        - 디스크가 망가지면 → Hadoop이 알아서 다른 노드 블록 복제본을 사용
        - RAID로 한 번 더 이중화하면 **중복투자** + **운영 복잡도** 증가
        
        즉, **RAID는 소규모/단일 서버 환경에서 유리**,
        
        **JBOD + HDFS 블록 복제는 대규모 분산 환경에서 유리**한 거예요.
        
    
    ### 1.2 클러스터 규모 산정
    
    고려해야하는 주요변수는 현재 보유 데이터의 크기, 데이터의 예상 증가율, 
    
    그리고 클러스터에서 수행될 작업의 종류와 빈도
    
    특히 저장 공간을 계산할 때는 HDFS의 복제 계수를 반드시 고려
    
    ex)
    
    기본값인 3으로 설정된 경우, 100TB의 원본 데이터를 저장하기 위해서는 약 300TB 물리적 저장 공간이 필요하다.
    
    초기 투자를 최소화하면 당장의 비용은 줄어들지만, 데이터가 예상보다 빠르게 증가할 경우 **노드 추가 작업**이 빈번해져 **운영 오버헤드** 급증할 수 있다.
    
    반대로 너무 큰 규모로 시작하면 상당 기간 **유휴 자원**으로 인한 **비용 낭비** 발생
    
    **이에 향후 6개월에서 1년의 데이터 증가율과 처리 요구사항을 예측하여 확장 단위를 미리 정의하면 좋다.**
    
    ex) 원본 데이터 100TB 증가 시, 5대의 워커 노드를 추가한다와 같은 명확한 정책 수립하면 예측하고 체계적인 클러스터 확장 가능해진다.
    
    ### 1.3 네트워크 토폴로지
    
    하둡 성능의 핵심은 cpu 속도가 아니라 **‘데이터 이동의 최소화**’에 있다.
    
    이를 구현하는 가장 중요한 기능이 바로 **‘랙 인식’**이다.
    
    하둡은 클러스터의 물리적 네트워크 구조, 즉 어떤 노드가 어떤 랙에 위치하는지 인지하고. 
    
    이를 데이터 배치와 작업 스케줄링에 적극적으로 활용한다.
    
    랙인식은 안정성과 성능을 동시에 달성하기 위한 목적이다.
    
    1. **안정성** : 최소 하나의 블록 복제본을 다른 랙에 위치시키도록 노력한다. → 하나의 랙에 장애 발생하더라도 가용성 보장된다.
    2. **성능** **최적화** : reduce의 shuffle 과정에서 막대한 양의 데이터가 네트워크 통해서 전송되는데 이때 가급적 동일한 랙 내에 있는 노드의 데이터를 우선적으로 처리하도록 task 배치한다. 이에 네트워크 비용 아낀다.
    
    [net.topology.script.file.name](http://net.topology.script.file.name) 속성을 통해 클러스터의 물리적 구조를 하둡에게 알린다. 
    
    → 해당 경로에 지정된 스크립트 실행하면서 노드의 호스트명이나 ip를 입력으로 넘긴다.
    
    - ex (GPT)
        
        core-site.xml
        
        ```xml
        <property>
          <name>net.topology.script.file.name</name>
          <value>/etc/hadoop/conf/topology.sh</value>
        </property>
        ```
        
        topology.sh
        
        ```kotlin
        #!/bin/bash
        
        # Hadoop이 IP/호스트명을 인자로 넘겨줌
        while read line; do
          case $line in
            node1|node2)
              echo "/rack1"
              ;;
            node3|node4)
              echo "/rack2"
              ;;
            *)
              echo "/default-rack"
              ;;
          esac
        done
        ```
        
    
    ## 2 단계별 클러스터 설치 및 구성
    
    ### 2.1 사전 준비 작업
    
    노드간 일관된 환경 구축 : java, unix 계정 설정, hadoop
    
    마스터 노드가 워커 노드에 비밀번호 없이 ssh 접근 가능하도록 설정
    
    ### 2.2 hadoop 설치 및 hdfs 초기화
    
    hadoop 바이너리를 모든 노드에 설치한 후, 사용 전에 namenode 반드시 포맷
    
    포맷작업은 새로운 빈 파일시스템을 생성하는 것으로, 저장 디렉터리와 네임노드 초기 버전의 영속적인 데이터 구조를 만든다.
    
    네임노드가 파일시스템의 모든 메타데이터를 직접 관리하고 데이터 노드는 클러스터에 동적으로 포함되거나 제외될 수 있기에 데이터 노드는 초기 포맷 과정에 전혀 관여하지 않는다.
    
    ### 2.3 데몬 관리
    
    hadoop 클러스터는 여러 데몬 프로세스들의 협력으로 동작한다.
    
    [start-dfs.sh](http://start-dfs.sh) 스크립트로 hdfs daemon (name node, data node, secondary namenode)
    
    [start-yarn.sh](http://start-yarn.sh) 스크립트로 yarn daemon (resource manager, node manager)
    
    [mr-jobhistory-daemon.sh](http://mr-jobhistory-daemon.sh) 스크립트로 mapreduce 작업 이력 서버 시작
    
    데몬 시작 순서에는 암묵적인 서비스 의존성 존재
    
    hdfs 가장 먼저 실행, 
    
    그 위에 리소스 관리 및 스케줄링 담당하는 yarn 동작 실행, 
    
    마지막으로 yarn위에 실제 데이터 처리 프레임워크인 mapreduce 작업 실행 
    
    (layered architecture)
    
    ex)
    
    Storage (HDFS) -> Resource Management (YARN) -> Processing Framework (MapReduce, Spark, etc.)
    
    ## 3 핵심 구성 속성
    
    hadoop의 동작은 xml 형식의 구성 파일들을 통해 제어된다.
    
    이 파일들은 클러스터의 성능, 안정성, 보안을 결정하는 중요한 변수들을 담고 있다.
    
    | 파일 이름 | 주요 역할 | 영향받는 데몬 |
    | --- | --- | --- |
    | `core-site.xml` | 클러스터 전역의 공통 설정 (예: 기본 파일시스템 URI) | 모든 데몬 |
    | `hdfs-site.xml` | HDFS 데몬(NameNode, DataNode) 관련 설정 | NameNode, DataNode, HDFS 클라이언트 |
    | `yarn-site.xml` | YARN 데몬(ResourceManager, NodeManager) 관련 설정 | ResourceManager, NodeManager, YARN 클라이언트 |
    | `mapred-site.xml` | MapReduce 애플리케이션 기본값 및 동작 방식 설정 | MapReduce 애플리케이션 |
    | `hadoop-env.sh` | Hadoop 데몬 실행에 필요한 환경 변수 설정 (예: `JAVA_HOME`) | 모든 데몬 |
    - **core-site.xml (책 예시)**
        
        fs.defaultFs 속성에 hdfs 파일 시스템의 url 지정한다.
        
        ![image.png](attachment:00aa194f-f8af-4f2b-bc69-0ffb9ed4c78f:image.png)
        
    
    ### 3.1 설정 관리 및 환경 변수
    
    hadoop은 단일한 전역 설정 위치 가지지 않으며, 각 노드는 자체 설정 파일을 가진다.
    
    이에 관리자는 파일들을 클러스터 전체에 동기화해야 하는 책임이 있다.
    
    cloudera manager나 apache ambari 같은 관리 도구로 자동화 가능하다.
    
    ex)
    
    `hadoop-env.sh` 파일은 데몬 실행에 필요한 환경 변수를 설정한다.
    
    - `JAVA_HOME`: 클러스터 전체에서 일관된 Java 버전을 사용하도록 명시적으로 설정하는 것이 권장된다.
    - **힙 크기 (**`HADOOP_HEAPSIZE`**)**: 각 데몬에 할당되는 메모리 양을 제어합니다. NameNode와 같이 메모리 요구량이 큰 데몬은 `HADOOP_NAMENODE_OPTS`를 통해 개별적으로 더 큰 힙 크기를 할당받아야 한다.
    - **로그 디렉토리 (**`HADOOP_LOG_DIR`**)**: 데몬 로그 파일의 위치를 지정한다. 설치 디렉토리 외부에 두어 업그레이드 시 로그가 보존되도록 하는 것이 좋다.
    - **SSH 설정 (**`HADOOP_SSH_OPTS`**)**: 클러스터 관리 스크립트가 사용하는 SSH 옵션을 사용자 정의할 수 있다. 예를 들어, 연결 타임아웃을 조정하거나 호스트 키 확인 정책을 변경할 수 있다.
    
    ### 3.2 hdfs 핵심 속성 (hdfs-site.xml)
    
    hadoop 근본적인 동작 방식 정의
    
    ex) dfs.replication : 데이터 블록을 몇 개의 복제본으로 유지할지 결정
    
    | 속성명 | 설명 | 기본값 | 권장 설정 |
    | --- | --- | --- | --- |
    | `dfs.namenode.name.dir` | NameNode가 메타데이터(FsImage, EditLog)를 저장할 로컬 디렉토리 경로. 쉼표로 구분하여 여러 경로 지정 가능. | 없음 | `/data/hadoop/nn` |
    | `dfs.datanode.data.dir` | DataNode가 데이터 블록을 저장할 로컬 디렉토리 경로. 쉼표로 구분하여 여러 디스크 지정 가능. | 없음 | `/data/hadoop/dn1,/data/hadoop/dn2` |
    | `dfs.replication` | HDFS에 저장되는 모든 파일의 기본 블록 복제 계수. | 3 | 프로덕션: 3, 개발/테스트: 1 |
    - **hdfs-site.xml (책 예시)**
        
        **dfs.namenode.name.dir** 속성에는 영속적인 파일시스템의 메타데이터(에디트 로그, 파일시스템 이미지)를 저장할 디렉토리 목록을 지정한다.
        
        이중화를 위해 속성에 나열된 **각 디렉터리에 복제본이 저장된다.**
        
        일반적으로 nfs로 마운트된 **원격 디스크 뿐만 아니라** 하나 이상의 **로컬 디스크**에 저장하도록 설정하는게 권장된다.
        
        이러면 로컬 장애가 발생하거나 네임노드 머신에 장애 발생해도 시스템 보호가능하다.
        
        **dfs.datanode.data.dir** 속성을 통해 데이터노드가 블록을 저장할 디렉터리 목록을 지정해야한다.
        
        데이터노드는 여러개의 저장 디렉터리에 라운드 로빈 방식으로 쓰기 작업을 한다.
        
        **dfs.namenode.checkpoint.dir** 속성을 통해 보조 네임노드가 파일시스템의 체크포인트를 저장할 곳을 설정해야 한다.
        
        메타데이터의 다중 복제본을 디렉터리에 각각 저장한다. (장애 대비용)
        
        ![image.png](attachment:2871bb13-46d4-46b4-9108-5820c736f5cd:image.png)
        
        ![image.png](attachment:fa75e4b9-df79-420b-aaa0-c6d01ba03401:image.png)
        
    
    ### 3.3 yarn 및 mapreduce 핵심 속성 (yarn-site.xml, mapred-site.xml)
    
    | 속성명 | 설명 | 파일 |
    | --- | --- | --- |
    | `mapreduce.framework.name` | MapReduce 작업을 실행할 프레임워크를 지정합니다. `yarn`으로 설정해야 합니다. | `mapred-site.xml` |
    | `yarn.resourcemanager.hostname` | 클러스터의 ResourceManager 데몬이 실행되는 호스트의 주소입니다. | `yarn-site.xml` |
    | `yarn.nodemanager.aux-services` | NodeManager에서 실행될 보조 서비스 목록. MapReduce를 사용하려면 `mapreduce_shuffle`을 반드시 포함해야 합니다. | `yarn-site.xml` |
    | `yarn.nodemanager.resource.memory-mb` | 각 NodeManager가 YARN에 할당할 수 있는 총 물리 메모리(MB)입니다. | `yarn-site.xml` |
    | `mapreduce.map.memory.mb` | Map 태스크를 실행하는 컨테이너에 할당될 메모리(MB)입니다. | `mapred-site.xml` |
    | `mapreduce.reduce.memory.mb` | Reduce 태스크를 실행하는 컨테이너에 할당될 메모리(MB)입니다. | `mapred-site.xml` |
    - **yarn-site.xml (책 예시)**
        
        yarn.resourcemanager.hostname : 리소스 매니저를 수행할 머신의 호스트명이나 ip 주소로 설정
        
        yarn.nodemanager.local-dirs : 맵리듀스 작업 수행중 중간 데이터 및 작업 파일을 저장할 임시 로컬 저장소 위치 제어 (라운드 로빈 방식으로 사용됨)
        
        yarn.nodemanager.aux-services : yarn은 맵리듀스 1과 달리 맵 출력을 리듀스 태스크에 제공하는 태스크트래커가 없어서 보조 서비스인 셔플 핸들러가 필요하다. 이에 명시적으로 작성하여 활성화해야한다. ex. mapreduce_shuffle
        
        ![image.png](attachment:942ecd15-49ba-452b-9fd5-0efc61581240:image.png)
        
    
    ### 3.4 다른 하둡 속성
    
    버퍼크기 : 하둡은 i/o 연산에 4KB 크기 버퍼 사용한다. 이는 보수적인 수치라 크기 증가시키면 성능상의 이점 가져갈 수 있다. 일반적으로 128KB 사용한다.
    
    hdfs 블록 크기 : 기본 블럭 크기 128MB지만 많은 클러스터가 더 큰 크기를 사용하여 네임노드 메모리 부담을 줄이면서, 매퍼가 더 많은 데이터를 처리할 수 있게 할 수 있다.
    
    휴지통 : 삭제된 파일이 영구적으로 제거되기 전까지 보관되는 시간을 분단위로 설정한다. 기본값 0으로 비활성화이다. 운영 환경에서는 활성화 하는게 좋다.
    
    단락 지역 읽기 : 읽어야 하는 블록이 클라이언트와 동일한 노드에 있다면 네트워크를 거치지 않고 디스크에서 직접 해당 블록 데이터를 읽는 것이 더 효과적이다. 이를 통해 성능 향상 이뤄낼 수 있다.
    
    잡 스케줄러
    
    클러스터 멤버십
    
    예약된 저장 공간
    
    느린 리듀스 시작
    
    ## 4 클러스터 보안 강화
    
    ### 4.1 Kerberos 통한 인증
    
    hadoop은 자체 사용자 자격 증명을 관리하는 대신, 성숙한 네트워크 인증 프로토콜인 kerberos에 의존하여 사용자 인증한다.
    
    core-site.xml 에서 kerberos 지정 및 auth 활성화, acl 정의 필요하다.
    
    - Kerberos란? (GPT)
        
        ## 🛡 Kerberos란?
        
        - **네트워크 인증 프로토콜** 중 하나로, 1980년대 MIT에서 개발됨.
        - 목적: **보안이 취약한 네트워크 환경에서 사용자가 신뢰할 수 있는 인증**을 제공하는 것.
        - 주로 **대규모 분산 환경(예: 기업 내부망, Windows Active Directory, Hadoop, Linux 서버 등)**에서 사용됨.
        
        ---
        
        ## 🔑 핵심 개념
        
        1. **대칭키 기반 프로토콜**
            - Kerberos는 기본적으로 암호화를 위해 대칭키(비밀키)를 사용함.
            - 비밀번호를 직접 전송하지 않고, 암호화된 "티켓(ticket)"을 교환해서 인증을 수행함.
        2. **티켓(ticket) 기반 인증**
            - 사용자가 서버에 접속할 때마다 비밀번호를 매번 입력하지 않고, 발급받은 티켓으로 인증 가능.
            - 일종의 "일회용 암호화된 신분증" 같은 개념.
        3. **신뢰할 수 있는 제 3자 (KDC)**
            - **KDC(Key Distribution Center)**: Kerberos의 핵심 서버
                - **AS(Authentication Server)**: 클라이언트의 신원을 확인하고 티켓 발급 시작
                - **TGS(Ticket Granting Server)**: 서비스 티켓을 발급
        
        ---
        
        ## ⚙️ 동작 과정 (단순화 버전)
        
        Kerberos 인증 절차는 보통 3단계로 나뉨:
        
        1. **인증 요청 (Authentication Service, AS)**
            - 사용자가 로그인 요청 → 비밀번호 기반 암호화로 **TGT(Ticket Granting Ticket)** 발급
            - 이때 비밀번호는 네트워크에 직접 전송되지 않음
        2. **서비스 티켓 요청 (TGS)**
            - 사용자가 어떤 서비스(DB, 파일 서버 등)에 접속하려고 하면 → TGT를 TGS에 제출
            - TGS는 사용자가 해당 서비스 접근 권한이 있는지 확인 후 **Service Ticket** 발급
        3. **서비스 접속**
            - 사용자는 Service Ticket을 서비스 서버에 제출
            - 서버는 이를 검증하고 세션 키(Session Key)를 공유 → 안전하게 통신 시작
        
        ---
        
        ## 📌 Kerberos의 장점
        
        - 비밀번호가 네트워크에 직접 노출되지 않음
        - 중간자 공격 방지 (티켓 기반 암호화)
        - 한 번 로그인하면 여러 서비스 이용 가능 (Single Sign-On, SSO 지원)
        
        ---
        
        ## ⚠️ 단점 / 고려사항
        
        - **KDC가 단일 장애점(Single Point of Failure)**
            
            → KDC가 죽으면 인증 자체가 불가능해짐
            
        - 시간 동기화 필수
            
            → Kerberos는 티켓에 유효기간을 두기 때문에, 서버와 클라이언트의 시간이 일정 범위 내에서 정확히 맞아야 함
            
        - 구현/운영이 복잡함
        
        ---
        
        ## 🏢 실제 사용 사례
        
        - **Microsoft Active Directory**: Windows 도메인 환경에서 기본 인증 프로토콜로 사용
        - **Hadoop**: 대규모 분산 파일 시스템(HDFS) 보안에 Kerberos 사용
        - **SSH, NFS 등 네트워크 서비스**에서도 Kerberos 기반 인증 가능
    
    ### 4.2 위임 토큰
    
    분산 시스템에서 모든 클라이언트-서버 상호작용을 kerberos로 인증하는 것은 key distribution center에 큰 부하를 줄 수 있다.
    
    이에 hadoop은 위임토큰 사용한다.
    
    ex)
    
    클라이언트가 서비스에 처음 접속할 때 kerberos로 인증하고 위임 토큰 발급받는다.
    
    이후 모든 요청에서는 해당 토큰 사용하여 kdc 거치지 않고 인증한다.
    
    특히 hdfs에서는 블록 데이터 접근을 위해 ‘블록 접근 토큰’이라는 특수 위임토큰 사용한다.
    
    namenode가 해당 토큰 발급하면, 클라이언트는 datanode에 이를 제시하여 블록에 대한 읽기/쓰기 권한 증명한다.
    
    ## 5 클러스터 성능 검증
    
    클러스터 구축 후에는 반드시 벤치마킹을 통해 시스템이 예상대로 동작하는지, 잠재적인 병목 지점은 없는지 검증해야한다.
    
    아래와 같은 벤치마크 결과 분석하여 클러스터 성능 특성 파악하고 최적화 방향 결정하는 기준 얻을 수 있다.
    
    ex)
    
    **TestDFSIO:** HDFS의 순수한 읽기/쓰기 성능을 측정한다. 이 벤치마크 결과가 기대에 미치지 못한다면, 네트워크 구성이나 디스크 I/O 시스템에 문제가 있을 가능성이 높다.
    
    **TeraSort:** 1TB의 데이터를 정렬하는 표준 벤치마크로, MapReduce 프레임워크의 전체 성능(I/O, CPU, 네트워크 Shuffle)을 종합적으로 측정한다. 특히 Shuffle 단계에서 시간이 오래 걸린다면, 이는 랙 인식 설정 오류와 같은 네트워크 문제나 디스크 I/O 성능 저하를 시사한다.
    
    **MRBench:** 작은 MapReduce 작업을 반복적으로 실행하여 YARN의 스케줄링 오버헤드와 작은 작업 처리 효율성을 평가한다.
