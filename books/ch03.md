# ch3. 하둡 분산 파일시스템
## HDFS 설계
### 매우 큰 파일
HDFS는 TB,PB급의 데이터도 저장할 수 있도록 설계되었다. 
### 스트리밍 방식의 데이터 접근
첫번 째 데이터를 읽는 시간보다 전체 데이터를 읽는 시간에서의 이점을 중요시하는 방향으로 설계되었다.
### 범용 하드웨어
고가의 하드웨어만을 고집하지 않아 노드에 장애가 발생해도 사용자가 장애가 발생했다는 사실조차 모르게 작업을 수행하도록 설계되었다. 
### 빠른 데이터 응답시간
수십 ms의 실시간 서비스급의 애플레이케이션을 HDFS와 맞지 않고 높은 데이터 처리량을 제공하기 위해 최적화 되었다. 
### 수많은 작은 파일
파일시스템의 메타 데이터를 메모리에서 관리하고 있다. 파일의 수는 메타 데이터를 관리하는 네임노드의 메모리 크기에 좌우된다. 
### 다중 라이터와 파일의 임의 수정
hdfs는 여러 사용자가 동시에 한 파일에 write할 수 없도록 설계되었고 파일의 중간을 수정할 수 없고 append만 가능하다. 

## HDFS 개념
### Block
- 디스크는 512B 단위로 데이터를 읽고 쓴다
- 대부분의 os file system은 4KB 단위로 데이터를 읽고 쓴다.
- HDFS는 128MB 단위로 데이터를 저장하고 처리한다.
  
#### 특징
| 항목          | 설명                                           |
| ----------- | -------------------------------------------- |
| 기본 크기    | **128MB** (클러스터 설정에 따라 더 크게 설정 가능)           |
| 저장 방식    | 파일은 여러 개의 블록으로 **쪼개서 저장**됨                   |
| 작은 파일    | 1MB 파일은 실제로 **1MB만 사용**됨 (128MB 블록이지만 낭비 없음) |
| 분산 저장    | 블록은 클러스터 내 여러 노드에 **분산 저장** 가능               |
| 복제       | 기본적으로 **3개 복제**되어 저장 → 내결함성 확보               |
| 블록 단위 처리 | MapReduce는 일반적으로 **블록 단위**로 작업을 처리           |

#### 장점
| 장점                 | 설명                                        |
| ------------------ | ----------------------------------------- |
| 대용량 파일 지원        | 블록 단위로 쪼개기 때문에 **한 디스크 용량보다 큰 파일도 저장 가능** |
| 분산 처리에 최적화       | 각 블록을 **여러 노드에 분산 저장**, 병렬로 처리 가능         |
| 복제 통한 내결함성       | 블록을 여러 노드에 복제해 **하드웨어 장애에도 안전**           |
| 메타데이터 단순화        | 블록은 단순한 데이터 조각이라, **파일 정보는 별도 메타데이터로 관리** |
| 블록 크기 고정 → 관리 용이 | 크기가 고정되어 있어 **저장소 계산과 관리가 용이**            |

## namenode와 데이터 노드
### namenode
| 역할                 | 설명                                                                                        |
| ------------------ | ----------------------------------------------------------------------------------------- |
| 파일 시스템 메타데이터 관리 | 전체 HDFS의 **디렉터리 트리**, 파일 이름, 퍼미션, 블록 ID 등을 관리                                             |
| 메타데이터 저장        | 로컬 디스크에 `fsimage`(네임스페이스 이미지)와 `edits`(수정 로그)로 저장                                         |
| 블록 위치 정보        | 블록의 위치(어느 Datanode에 있는지)는 메모리에 유지, **재시작 시 Datanode로부터 복구**                               |
| 중요성              | **없으면 클러스터 전체를 사용할 수 없음** → 데이터 복구 불가                                                     |
| 내결함성 방안        | - 메타데이터를 **NFS 등 다른 위치에 동기식으로 백업**<br>- Secondary Namenode 또는 HA(High Availability) 구성 사용 |
### datanode
| 역할             | 설명                                     |
| -------------- | -------------------------------------- |
| 실제 데이터 저장   | Namenode의 명령에 따라 블록을 **저장 및 제공**       |
| 정기 보고       | 자신이 보유한 블록 목록을 **주기적으로 Namenode에 보고**  |
| 클라이언트 직접 통신 | 클라이언트는 실제 데이터를 **Datanode로부터 직접 읽음/씀** |

### Secondary Namenode
| 항목             | 설명                                                        |
| -------------- | --------------------------------------------------------- |
| 실시간 백업 서버 아님 | Namenode가 죽으면 자동 전환되는 **Hot Standby** 역할 아님               |
| 역할           | `fsimage + edits`를 주기적으로 **merge**하여 edit log가 커지지 않도록 유지 |
| 복구 방법      | 비정상 종료 시 **Secondary의 병합 이미지나 NFS 백업으로 수동 복구** 필요         |
현재는 namenode HA로 인해 거의 사용하지 않음

### namenode HA
기존 HDFS에서는 Namenode가 단일 장애 지점이었다. 이를 해결하기 위해  hdfs 2버전부터 namenode HA기능을 제공한다. 
| 구성요소                    | 역할                                              |
| ----------------------- | ----------------------------------------------- |
| **Active Namenode**     | 클러스터의 요청을 실제로 처리하는 **실행 중인 Namenode**           |
| **Standby Namenode**    | Active와 동일한 메타데이터를 유지하면서 **대기 중인 Namenode**     |
| **JournalNode (3대 이상)** | 두 Namenode 간 **Edit log를 공유**하는 중간 저장소          |
| **ZooKeeper**           | Active/Standby 상태를 조정하고 **failover 감지 및 전환 관리** |

## 블록 캐싱
자주 읽히는 데이터는 disk read를 수행하지 않고 메모리에 캐싱해둔다. 
| 항목          | 설명                                                  |
| ----------- | --------------------------------------------------- |
| 위치       | DataNode의 **off-heap 메모리 영역**에 블록을 저장               |
| 기본 캐싱 수  | **기본적으로 1개 DataNode만** 해당 블록을 캐싱함 (per-file로 조정 가능) |
| 활용 예시    | 작은 lookup 테이블 등 자주 읽는 데이터                           |
| 캐시 지속 시간 | 사용자가 **명시적으로 지정** 가능 (TTL처럼)                        |

## HDFS 페더레이션 
HDFS Federation은 네임노드의 확장을 통해 클러스터의 확장성과 독립성을 높이는 구조다. 기존 HDFS는 단일 NameNode만 있어 확장성과 가용성에 한계가 있었지만,Federation은 여러 NameNode를 병렬로 구성해 이를 해결한다.

### 필요한 이유
- NameNode는 모든 파일과 블록 메타데이터를 메모리에 보관함 → 파일 수가 많아질수록 메모리 한계
- 단일 NameNode가 커지면 성능 저하 및 SPOF(Single Point of Failure) 위험

### 구조
| 구성 요소                       | 설명                                                                  |
| --------------------------- | ------------------------------------------------------------------- |
| **여러 NameNode**             | 각각 **독립적인 namespace volume**을 관리 (/user, /share 등 경로별 분할)           |
| **Namespace Volume**        | 하나의 NameNode가 관리하는 파일 시스템 영역 (ex: /user, /data 등)                   |
| **Block Pool**              | 해당 namespace의 블록 데이터 집합                                             |
| **DataNode**                | 모든 NameNode에 등록되며, **여러 Block Pool의 블록**을 저장 가능                     |
| **ViewFileSystem (ViewFs)** | 클라이언트가 여러 namespace를 **통합된 파일시스템처럼 보도록** 해주는 설정 방식 (`viewfs://` 사용) |

## HDFS HA
NameNode 단일 장애점(SPOF, Single Point of Failure) 문제를 해결하기 위해 활성-대기(active-standby) 구조로 NameNode를 이중화(failover) 하는 기능

### 목적
- HDFS의 NameNode는 모든 메타데이터(파일 위치, 디렉터리 구조 등)를 관리하는 핵심
- 기존엔 NameNode가 죽으면:
  - 읽기/쓰기 모두 중단
  - 클러스터 전체가 멈춤
  - 복구 시간 30분 이상 걸릴 수 있음 
- HA는 장애 시 빠르게 자동 전환(failover) 하여 중단 시간 최소화

## 구송 요소 및 메커니즘
| 구성 요소                          | 설명                                                                     |
| ------------------------------ | ---------------------------------------------------------------------- |
| **Active NameNode**            | 클라이언트 요청을 처리하는 **실제 동작 중인 NameNode**                                   |
| **Standby NameNode**           | 요청은 받지 않고, **메타데이터 동기화 유지**                                            |
| **Shared Storage**             | **Edit log를 공유**하기 위한 저장소<br>① NFS 또는 ② Quorum Journal Manager(QJM) 사용 |
| **JournalNode (QJM)**          | edit log를 복제 저장하는 **3개의 데몬 프로세스**, 다수 쓰기 승인 구조                         |
| **ZooKeeper**                  | **Failover 제어** 및 **Active 선출**                                        |
| **Failover Controller (ZKFC)** | 각 NameNode 옆에서 실행되며, 상태 감시 + 장애 전환                                     |

## 동작과정
1. Active NN이 edit log 작성
2. Standby NN은 QJM에서 실시간으로 edit log 읽음
3. Datanode는 두 NN에 block report 전송
4. Active 장애 발생 시:
   - ZKFC가 감지
   - ZooKeeper 통해 Standby가 Active로 승격
   - 중단 시간 수 초~1분 이내

## fencing
장애가 난 이전 Active Namenode가 잘못 동작하지 않도록 차단하는 것

### 수행방법
- SSH로 프로세스 kill (ssh root@host 'kill -9 PID')
- NFS 접근 차단
- 네트워크 인터페이스 비활성화
- STONITH (Shoot The Other Node In The Head): 전원 강제 차단

# HDFS CLI
- HDFS를 명령어 기반으로 다루는 가장 기본적이고 직관적인 방법
- 익숙한 ls, cp, mkdir 등과 유사한 형태로 HDFS 파일 조작 가능

## 기본 명령어
### 파일 업로드 (로컬 -> HDFS)
```
hadoop fs -copyFromLocal input.txt /user/tom/input.txt
```
### 파일 다운로드 (HDFS -> 로컬)
```
hadoop fs -copyToLocal /user/tom/input.txt ./input.copy.txt
```
### 디렉토리 생성 및 목록 확인
```
hadoop fs -mkdir books
hadoop fs -ls .

drwxr-xr-x - tom supergroup 0 2023-07-27 10:12 books
-rw-r--r-- 1 tom supergroup 119 2023-07-27 10:11 quangle.txt
```

## HDFS 파일 권한 모델
| 권한            | 의미                     |
| ------------- | ---------------------- |
| `r` (read)    | 파일 읽기 / 디렉토리 리스트 보기    |
| `w` (write)   | 파일 수정 / 디렉토리에 파일 생성/삭제 |
| `x` (execute) | 디렉토리 접근 허용 (파일엔 의미 없음) |

POSIX와 유사하게 3가지 권한으로 사용자를 관리할 수 있고 hadoop security를 설정해야한다. 

## Hadoop Filesystem
- Hadoop은 추상화된 파일시스템 인터페이스를 제공하며, org.apache.hadoop.fs.FileSystem이 그 추상 클래스입니다.
- 여러 종류의 파일시스템이 Hadoop에 플러그인 방식으로 구현되어 있음.
- 대부분은 URI 스킴을 통해 자동 인식 및 연동됨.


| Filesystem               | URI 스킴        | Java 구현 클래스                              | 설명                                                                   |
| ------------------------ | ------------- | ---------------------------------------- | -------------------------------------------------------------------- |
| **Local**                | `file:///`    | `fs.LocalFileSystem`                     | 로컬 디스크용 파일시스템. 기본적으로 체크섬 지원. <br> (`RawLocalFileSystem` 사용 시 체크섬 없음) |
| **HDFS**                 | `hdfs:///`    | `hdfs.DistributedFileSystem`             | Hadoop의 기본 분산 파일시스템. <br> MapReduce와 최적화되어 연동                        |
| **WebHDFS**              | `webhdfs://`  | `hdfs.web.WebHdfsFileSystem`             | HTTP로 인증된 HDFS 접근 제공                                                 |
| **Secure WebHDFS**       | `swebhdfs://` | `hdfs.web.SWebHdfsFileSystem`            | HTTPS 기반 WebHDFS                                                     |
| **HAR (Hadoop Archive)** | `har://`      | `fs.HarFileSystem`                       | 많은 파일을 하나의 아카이브로 묶는 파일시스템. <br> Namenode 메모리 절약용                     |
| **ViewFS**               | `viewfs://`   | `viewfs.ViewFileSystem`                  | 여러 파일시스템을 하나로 묶는 **클라이언트 측 마운트 테이블**. <br> HDFS Federation과 자주 사용    |
| **FTP**                  | `ftp://`      | `fs.ftp.FTPFileSystem`                   | FTP 서버 기반 파일시스템                                                      |
| **Amazon S3**            | `s3a://`      | `fs.s3a.S3AFileSystem`                   | Amazon S3 연동용. <br> 구식 `s3n`보다 빠르고 안정적                               |
| **Azure**                | `wasb://`     | `fs.azure.NativeAzureFileSystem`         | Microsoft Azure Storage 지원                                           |
| **Swift**                | `swift://`    | `fs.swift.snative.SwiftNativeFileSystem` | OpenStack Swift 연동                                                   |

## Interface
하둡은 Java로 작성되었기 때문에 자바 API를 통해 HDFS와 연동할 수 있다. C, NFS, FUSE, HTTP 방식으로도 제공한다.

### HttpFs(proxy방식의 WebHDFS)
자바로 작성되지 않은 애플리케이션으로 HDFS에 접근할 수도 있다. 하지만 성능이 느리므로 매우 큰 파일을 전송할 때 주의해야한다.
![](https://velog.velcdn.com/images/fadfa/post/6f7a343d-15d1-4387-90c4-d9d5990e4cbb/image.png)

| 항목    | 설명                                           |
| ----- | -------------------------------------------- |
| 역할    | 클러스터 외부에서 HTTP로 HDFS 접근 가능하게 해주는 **프록시 서버**  |
| 동작 방식 | 클라이언트 → HttpFS 프록시 → Namenode/DataNode       |
| 장점    | - **로드 밸런싱**, **Firewall 보안**, **멀티테넌시**에 유리 |
| 활용 사례 | - **다른 데이터센터** 간의 통신                         |

### Java
#### Read
##### URLStreamHandlerFactory
```java
InputStream in = new URL("hdfs://host/path").openStream();
```
- JVM에서 한 번만 설정 가능
- 외부 라이브러리와 충돌할 수 있으므로 FileSystem API를 권장
  
##### FileSystem API
```java
FileSystem fs = FileSystem.get(new URI(uri), conf);
FSDataInputStream in = fs.open(new Path(uri));
```
- seek() 사용해 랜덤 액세스 가능
- PositionedReadable 인터페이스로 특정 위치에서 데이터 읽기 가능

#### Write
``` java
FSDataOutputStream out = fs.create(new Path(uri));
```
##### Progressable Callback
``` java
fs.create(new Path(uri), new Progressable() {
  public void progress() {
    System.out.print(".");
  }
});
```
콜백 인터페이스를 넘겨줄 수 있어서 진행상황을 계속 파악할 수도 있다. 
##### append
```java
fs.append(new Path(uri));
```
- HDFS는 지원 S3는 불가

#### 디렉토리 생성
```java
fs.mkdirs(new Path("/some/dir"));
```
- 부모 디렉토리도 자동 생성됨

#### 파일 메타정보 조회 (FileStatus)
```java
FileStatus status = fs.getFileStatus(new Path(uri));
```
- 길이, 블록 크기, 권한, 소유자 등 조회 가능

#### 디렉터리 목록 조회
```java
FileStatus status = fs.getFileStatus(new Path(uri));
```

#### glob 패턴 (파일 패턴 매칭)
```java
fs.globStatus(new Path("/2007/*/31"));
```
- *, ?, [a-b], {a,b} 등의 Unix 스타일 와일드카드 지원

##### PathFilter
```java
public class RegexExcludePathFilter implements PathFilter {
  public boolean accept(Path path) {
    return !path.toString().matches("정규표현식");
  }
}
```
- glob으로는 한계가 있을 경우 별도의 PathFilter를 생성하여 구현

#### 삭제
```java
fs.delete(new Path("/some/path"), true);
```

## 데이터 흐름
### 네트워크 토폴로지와 하둡
- 가깝다는 것은 대역폭이 높다는 의미
- 클러스터 네트워크 구조를 트리로 추상화함
![](https://velog.velcdn.com/images/fadfa/post/521a3419-6a7a-4357-972d-4084d9ec241b/image.png)
| 예시                   | 의미             | 거리 |
| -------------------- | -------------- | -- |
| /d1/r1/n1, /d1/r1/n1 | 동일 노드          | 0  |
| /d1/r1/n1, /d1/r1/n2 | 동일 랙           | 2  |
| /d1/r1/n1, /d1/r2/n3 | 동일 데이터센터, 다른 랙 | 4  |
| /d1/r1/n1, /d2/r3/n4 | 다른 데이터센터       | 6  |


### File Read
![](https://velog.velcdn.com/images/fadfa/post/175ef86e-6a20-469f-81e3-e08628b58504/image.png)
1. Client → FileSystem.open(path) 호출
- 내부적으로 DistributedFileSystem 객체를 통해 파일 열기 요청
- Namenode에 첫 번째 몇 개의 블록 위치 요청
2. Namenode → 블록 위치 정보 반환
- 각 블록에 대해 복제본을 가진 Datanode들의 주소를 반환
- 클라이언트와 가까운 순서대로 정렬되어 있음
- 클라이언트가 Datanode인 경우(local datanode) → Short-circuit read 수행 가능
3. Client ← FSDataInputStream 반환
- 이 객체는 내부적으로 DFSInputStream을 래핑함
- DFSInputStream이 Datanode와의 통신 관리
4. Client → read() 호출 → Datanode에서 블록 데이터 전송
- 하나의 블록이 끝나면 Datanode 연결 닫고 다음 Datanode에 연결
5. DFSInputStream이 다음 블록 위치 재요청
- 필요 시 Namenode에 추가 블록 위치 요청
6. Client → close() 호출로 읽기 종료

### File Write
![](https://velog.velcdn.com/images/fadfa/post/f350a5e6-286e-457c-ac2a-ab9501c3156f/image.png)
1. 클라이언트 → create() 호출
- DistributedFileSystem.create() 호출
- Namenode에 새 파일 생성 요청
  - 파일이 이미 있는지 확인
  - 권한 체크
- 빈 블록 리스트로 메타데이터 등록

2. 클라이언트 ← FSDataOutputStream 반환
- DFSOutputStream을 내부적으로 포함하고 있어 datanode와의 통신 담당

3. 클라이언트 → write() 호출
- DFSOutputStream은 데이터를 패킷(packet) 으로 나누고 data queue에 저장
- DataStreamer가 이를 읽어서 Namenode에 새 블록 요청 및 데이터노드 목록 할당

4. DataStreamer → Datanode 파이프라인 구성 및 전송
- 예: 복제 3이라면 D1 → D2 → D3 파이프라인 구성

- 각 Datanode는:
  - 패킷을 저장
  - 다음 Datanode로 전달

5. Ack Queue 처리
- DFSOutputStream은 패킷을 ack queue에 보관
- 모든 Datanode로부터 ACK 수신 시에만 제거

- 실패 감지 시:
  - 파이프라인 중단
  - ack queue → data queue로 되돌려 재전송
  - 실패 노드 제거 후 새 파이프라인 구성
  - 남은 블록은 정상 노드에 이어서 작성
  - Namenode는 복제 부족 감지 후 비동기로 추가 복제 진행
  - 다중 노드 장애 시 처리
    - 설정값 dfs.namenode.replication.min (기본값 1) 개수 이상 복제에 성공하면 write 성공
    - 이후 비동기적으로 정상 복제수(dfs.replication = 기본 3) 까지 맞춤

6. 클라이언트 → close() 호출
- 남은 패킷을 모두 파이프라인으로 전송
- 모든 ACK 수신 후 → Namenode에 파일 쓰기 완료 알림

7. Namenode → 파일 완성 처리
- Namenode는 블록 리스트와 복제 상태를 이미 알고 있음
- 모든 블록이 최소 복제수 이상이면 정상 완료 응답

### HDFS Replica Placement
HDFS는 데이터의 안정성, 쓰기 성능, 읽기 성능 간의 균형을 맞추기 위해 복제본을 적절한 노드에 분산하여 저장합니다. 이 때 복제본을 어디에 저장해둘지에 대한 전략에 대해 알아봅니다.

#### 기본 복제 전략
- 첫 번째 복제본
 - 클라이언트와 동일한 노드에 저장
- 두 번째 복제본
  - 다른 랙에 있는 임의의 노드에 저장
- 세 번째 복제본 
  - 두 번째 복제본과 같은 랙의 다른 노드에 저장
- 이 후 추가 복제본
  - 클러스터 내 임의의 노드에 저장

### hdfs 일관성 모델
- 파일 생성 후 즉시 visible: fs.create(path) 후 fs.exists(path)는 true를 반환함 → 파일은 네임노드에 등록되어 있음.
- flush만으로는 내용이 안 보일 수 있음: out.flush()를 해도 클라이언트가 작성한 데이터는 다른 클라이언트에게 보이지 않음 → 블록 단위로 처리되기 때문.
- 최소 1 block이 넘어야 일부 내용이 visible: 현재 쓰고 있는 블록은 다른 클라이언트에게 보이지 않음, 완료된 블록만 읽기 가능.
| 메서드        | 설명                                                                                |
| ---------- | --------------------------------------------------------------------------------- |
| `hflush()` | 현재까지 쓴 데이터를 **모든 파이프라인 datanode의 메모리까지 전파**. 다른 클라이언트가 읽을 수 있게 됨. (디스크 flush는 아님) |
| `hsync()`  | `hflush()`와 동일 + 데이터가 **디스크까지 플러시**됨 → 디스크 장애/정전에도 대비 가능.                         |
| `close()`  | 암시적으로 `hflush()`를 호출함.                                                            |

### distcp
- distcp는 대규모 파일과 디렉터리 트리를 병렬로 복사하기 위한 MapReduce 기반 유틸리티
- 클러스터 간, 혹은 동일 클러스터 내 복사 작업에 적합
- 내부적으로 맵리듀스(실제로는 맵만) 작업이 수행되고 맵의 작업 수행 개수를 적게 주면 특정 노드에 데이터가 몰릴 수 있어서 hdfs balancer를 수행해야한다.

```java
hadoop distcp hdfs://source-cluster:8020/source/path hdfs://dest-cluster:8020/dest/path
```
