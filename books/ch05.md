# ch.5 하둡 I/O
하둡은 데이터 I/O를 위한 프리미티브를 제공한다.

## 데이터 무결성
손상된 데이터를 검출하는 일반적인 방법
- 데이터가 시스템에 처음 유입되었을 때와 데이터를 손상시킬지도 모르는 신뢰할 수 없는 통신 채널로 데이터가 전송되었을 때마다 체크섬을 계산한다.
  - 만약 새롭게 생성된 체크섬이 원본과 정확히 일치하지 않다면 그 데이터는 손상된 것으로 간주한다.
  - 이 방법은 데이터를 원상 복구하는 방법을 제공하지 않고 에러 검출만 수행한다.

### HDFS의 데이터 무결성
HDFS는 모든 데이터를 쓰는 과정에서 내부적으로 체크섬을 계산하고, 데이터를 읽는 과정에서 체크섬을 기본으로 검증한다.

- 데이터 노드는 데이터와 체크섬을 저장하기 전에 수신한 데이터를 검증할 책임이 있다.
- 클라이언트는 데이터노드에 저장된 체크섬과 수신된 데이터로부터 계산된 체크섬을 검증한다.

클라이언트의 읽기 과정에서 블록을 검증하는 것 외에도 각 데이터노드는 저장된 모든 블록을 주기적으로 검증하는 DataBlockScanner를 백그라운드 스레드로 수행한다.

 => 물리적 저장 매체에서 발생할 수 있는 **비트 로트** 에 의한 데이터 손실을 피하기 위한 방법.

### LocalFileSystem
- filename이라는 파일을 쓸 때 파일 시스템 클라이언트는 파일과 같은 위치의 디렉터리에 그 파일을 각 청크별 체크섬이 담긴 .filename.crc라는 파일을 내부적으로 생성한다. 
- 체크섬이 검증되고 에러가 검출되면 LocalFileSystem이 **ChecksumException**을 발생한다.

기존 파일시스템이 자체적으로 체크섬을 지원한다면 LocalFileSystem의 체크섬을 비활성화할 수 있다.
(LocalFileSystem 대신 RawLocalFileSystem 사용)

### ChecksumFileSystem
이 클래스는 단순한 FileSystem의 래퍼로 구현되어 있기 때문에 다른 체크섬이 없는 파일시스템에 체크섬 기능을 쉽게 추가할 수 있다.
```
FileSystem rawFs = ...
FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
```

- 원시 파일시스템 : 내부의 파일시스템.

#### ChecksumFileSystem의 메서드들
| 메서드 | 동작 |
| --------- | ---|
| getRawFileSystem()   | 원시 파일시스템을 얻을 수 있음.      |
| getChecksumFile()      |어떤 파일에 대한 체크섬 경로를 얻을 수 있음.    |

ChecksumFileSystem이 파일을 읽을 때 에러를 검출하면 reportChecksumFailure() 메서드를 호출한다.

## 압축
파일 압축의 이점
1. 파일 저장 공간을 줄일 수 있다.
2. 네트워크 또는 디스크로부터 데이터 전송을 고속화할 수 있다.


#### 압축 포맷의 요약
| 압축 포맷 | 도구  | 알고리즘 | 파일 확장명 | 분할 가능 |
| --------- | ------|---------|---------------|-----------|
| DEFLATE   | N/A   | DEFLATE | .deflate      | No        |
| gzip      | gzip  | DEFLATE | .gz           | No        |
| bzip2     | bzip2 | bzip2   | .bz2          | Yes       |
| LZO       | lzop  | LZO     | .lzo          | No        |
| LZ4       | N/A   | LZ4     | .lz4          | No        |
| Snappy    | N/A   | Snappy  | .snappy       | No        |

- 모든 압축 알고리즘은 공간과 시간이 트레이드오프 관계.
- 위의 표에 있는 도구들은 보통 9개의 옵션을 제공한다.(-1: 스피드 최적화, -9: 공간 최적화) 

<가장 빠른 압축 메서드를 사용해 file.gz 압축 파일 생성.>
```
% gzip -1 file
```

#### 압축 포맷 특성
| 압축 포맷 | 압축 속 | 압축 효율성 | 압축 해제 속도 |
| --------- | ------|---------|---------------|
| gzip      | 기본  | 기본    |  기본       |
| bzip2     | 느림  | 좋음    | 느림        |
| LZO       | 빠름  | 낮음  | 빠름        |
| LZ4       | 매우 빠름  | 낮음  | 매우 빠름   |
| Snappy    | 매우 빠름  | 낮음  | 매우 빠름   |

### 코덱
코덱은 압축-해제 알고리즘을 구현한 것이다.

#### 하둡 압축 코덱
| 압축 포맷 | 하둡 압축 코덱                            |
| --------- | -------------------------------------------|
| DEFLATE   | org.apache.hadoop.io.compress.DefaultCodec |
| gzip      | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2     | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO       | com.hadoop.compression.lzo.LzopCodec       |
| LZ4       | org.apache.hadoop.io.compress.Lz4Codec     |
| Snappy    | org.apache.hadoop.io.compress.SnappyCodec  |

**CompressionCodec을 통한 압축 및 해제 스트림**

CompressionCodec은 데이터를 쉽게 압축하거나 해제해주는 두 개의 메서드를 제공한다.

| 메서드 | 동작                          |
| --------- | -------------------------------------------|
| createOutputStream(OutputStreamout out)   | 압축되지 않은 데이터를 압축된 형태로 내부 스트림에 쓰는 CompressionOutputStream을 생성. |
| createInputStream(InputStream in)       | 기존 스트림으로부터 비압축 데이터를 읽을 수 있는 CompressionInputStream을 반환. |


**CompressionCodecFactory를 사용하여 CompressionCodec 유추하기**

압축된 파일을 읽을 때 해당 파일 확장명을 보면 사용한 코덱을 유추할 수 있다.

#### CompressionCodecFactory의 메서드
- getCodec() : 지정된 파일에 대한 path 객체를 인자로 받아 파일 확장명에 맞는 CompressionCodec을 찾아줌.
- removeSuffix() : 코덱을 찾았으면 출력 파일의 이름을 생성하기 위해 파일 접미사 제거.


#### 압축 코덱 속성
| 속성명               | 타입                     | 기본값 | 설명                                                             | 
| -------------------- | -------------------------|--------|-------------------------------------------------------------------|
| io.compression.codec | 콤마로 구분된 클래스 이름 |        | 압축 및 해제를 위해 추가하고자 하는 CompressionCodec 클래스 목록   |

-CompressionCodecFactory는 이전 표에 나열된 코덱(LZO 제외)과 현재 표 속성에 나열된 코덱을 불러온다.


**원시 라이브러리**
성능 관점에서 압축과 해제를 위해 원시 라이브러리를 사용하는 것이 바람직하다.

#### 압축 라이브러리 구현체
| 압축 포맷 | 자바 구현체 | 원시 구현체 | 
| --------- | -------------|-------------|
| DEFLATE   | Yes          | Yes         |
| gzip      | Yes          | Yes         |
| bzip2     | Yes          | Yes         |
| LZO       | No           | Yes         |
| LZ4       | No           | Yes         |
| Snappy    | No           | Yes         |

**코덱 풀** : 압축기와 해제기를 재사용해서 객체 생성 비용을 절감할 수 있다.


### 압축과 입력 스플릿
<어떤 압축 포맷을 사용해야 하는지 대략적인 제안을 효율성 순으로 소개>
- 압축과 분할 모두를 지원하는 컨테이너 파일 포맷을 사용하라. (LZ, LZ4, Snappy 같은 빠른 압축 형식)
- 분할을 지원하는 압축 포맷을 사용하라.(bzip2 ) / 혹은 분할을 지원하기 위해 색인될 수 있는 포맷을 사용하라. (LZO)
- 파일을 청크로 분할하고 지원되는 모든 압축 포맷을 사용해 각 청크를 개별적으로 압축하라.
- 파일을 압축하지 말고 그냥 저장하라.


### 맵리듀스에서 압축 사용하기

#### 맵리듀스 압축 속성
| 속성명                                           | 타입      | 기본값                                     | 설명                                                 | 
| ------------------------------------------------ | ----------|--------------------------------------------|-------------------------------------------------------|
| mapreduce.output.fileoutputformat.compress       | 불린      | false                                      | 출력 압축 여부                                        |
| mapreduce.output.fileoutputformat.compress.codec | 클래스명  | org.apache.hadoop.io.compress.DefaultCodec | 출력 압축에 사용할 코덱                                |
| mapreduce.output.fileoutputformat.compress.type  | 문자열    | RECORD                                     | 순차 파일 출력에 사용할 압축 유형(NONE, RECORD, BLOCK) |

 <맵리듀스 잡의 출력을 압축하려면>
- mapreduce.output.fileoutputformat.compress 속성을 true로 설정하고,
- 사용할 압축 코덱의 클래스 이름을 mapreduce.output.fileoutputformat.compress.codec에 지정하기.


**맵 출력 압축**
LZO, LZ4, Snappy 같은 빠른 압축기를 사용하여 전송할 데이터양을 줄이면 성능을 향상시킬 수 있다.

#### 맵 출력 압축 속성
| 속성명                              | 타입      | 기본값                                    | 설명                      | 
| ----------------------------------- | ----------|--------------------------------------------|---------------------------|
| mapreduce.map.output.compress       | 불린      | false                                      | 맵 출력의 압축 여부       |
| mapreduce.map.output.compress.codec | 클래스명  | org.apache.hadoop.io.compress.DefaultCodec | 맵 출력에 사용할 압축 코덱 |


## 직렬화
- 직렬화 : 네트워크 전송을 위해 구조화된 객체를 바이트 스트림으로 전환하는 과정.
- 역직렬화 : 바이트 스트림을 일련의 구조화된 객체로 역전환하는 과정.

<직렬화가 나타나는 영역>
1. 프로세스 간 통신
- **원격 프로시저 호출(RPC)** 를 사용하여 구현.
- RPC 직렬화 포맷이 유익한 이유
  - 간결성 : 간결한 포맷을 사용하면 네트워크 대역폭을 절약할 수 있다.
  - 고속화 : 분산 시스템의 핵심인 프로세스 간 통신에서 오버헤드를 줄여 성능을 높일 수 있다.
  - 확장성 : 다양한 데이터 타입과 구조를 유연하게 처리할 수 있어 시스템 확장에 용이하다.
  - 상호운용성 : 다양한 언어로 작성된 클라이언트를 지원하기 위해 포맷을 설계할 필요가 있다.

2. 영속적인 저장소
- 영속적 데이터는 저장 후에도 수년 동안 읽힐 수 있다.
- 따라서 하둡은 Writable이라는 간결하고 빠른 직렬화 포맷을 사용한다.

### Writable 인터페이스
<자신의 상태 정보를 DataOutput 바이너리 스트림으로 쓰기 위한 메서드 & DataInput 바이너리 스트림으로부터 상태 정보를 읽기 위한 메서드 정의>

```
package org.apache.hadoop.io;

import java.io.DataOutput;
import java.io.DataInput;
import java.io.IOException;

public interface Writable {
  void write(DataOutput out) throws IOException;
  void readFields(DataInput in) throws IOException;
}
```
이것을 활용하는 방법을 알아보기 위해 특정 writable 살펴보기.

<직렬화>
- 인스턴스를 하나 생성하고 set() 메서드로 값 설정.
```
InWritable writable = new IntWritable();
writable.set(163);
```

- 값 직접 설정도 가능.
```
InWritable writable = new IntWritable(163);
```

- InWritable의 직렬화된 형태 검증을 위해 헬퍼 메서드를 작성하고 직렬화된 스트림에서 바이트 추출.
```
public static byte[] serialize(Writable writable) throws IOException {
  ByteArrayOutputStream out = new ByteArrayOutputStream();
  DataOutputStream dataOut = new DataOutputStream(out);
  writable.writable(dataOut);
  dataOut.close();
  return out.toByteArray();
}
```

<역질렬화>
- 바이트 배열로부터 Writable 객체를 읽는 헬퍼 메서드를 만들기
```
public static byte[] deserialize(Writable writable, byte[] bytes) throws IOException {
  ByteArrayInputStream in = new ByteArrayInputStream();
  DataInputStream dataIn = new DataInputStream(in);
  writable.readFields(dataIn);
  dataIn.close();
  return bytes;
}
```
- get() 메서드를 사용해 얻은 값이 원래 값인 163인지 검사.
```
IntWritable newWritable = new IntWritable();
deserialize(newWritable, bytes);
asserrThat(newWriteable.get(), is(163));
```

**WritableComparable과 비교자**

- java.lang.Comparable 인터페이스의 서브인터페이스인 WritableComprarable
```
package org.apache.hadoop.io;

public interface WritableComparable<T> extends Writable, Comparable<T> {
}
```

- 하둡은 자바의 Comparator를 확장한 최적화된 RawComparator를 제공.
```
package org.apache.hadoop.io;

import java.util.Comparator;

public interface RawComparator<T> extends Comparator<T> {
  public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);
}
```

- 이 인터페이스는 레코드를 객체로 역직렬화하지 않고 스트림에서 읽은 레코드를 **구현자가 직접 비교**할 수 있도록 지원함으로써 객체 생성에 수반되는 오버헤드를 피할 수 있다.

### Writable 클래스
하둡은 org.apache.hadoop.io 패키지에서 많은 writable 클래스를 제공.

<img width="595" height="858" alt="image" src="https://github.com/user-attachments/assets/5d2c6408-21cb-44dc-a2dc-ead07d86628c" />

**자바 기본 자료형을 위한 Writable 래퍼**

모든 writable 래퍼는 래핑된 값을 얻고 저장하기 위한 get()과 set() 메서드를 제공한다.

| 자바 기본 자료형 | Writable 구현체              | 직렬화된 크기(바이트) | 
| ----------------- | -----------------------------|-----------------------|
| boolean           | BooleanWritable              | 1                     |
| byte              | ByteWritable                 | 1                     |
| short             | ShortWritable                | 2                     |
| int               | intWritable / VintWritable   | 4 / 1~5               |
| float             | FloatWritable                | 4                     |
| long              | longWritable / VlongWritable | 8 / 1~9               |
| double            | DoubleWritable               | 8                     |

<고정 길이와 가변길이 인코딩을 선택하는 기준>
- 고정길이 인코딩 : 값의 전체 공간에서 값이 매우 균일하게 분포되어 있을 때 적합.
- 가변길이 인코딩 : 숫자 값은 대부분 균일하지 않게 분포되어 있으므로 이럴 때 적합.


**텍스트**
Text는 UTF-8 시퀀스를 위한 Writable 구현체.
  - 가변길이 인코딩으로 int 사용. (문자열 인코딩에 다수의 바이트를 저장하기 위해)

- 인덱스 만들기.
  - charAt() 메서드를 사용하는 예제.
    ```
    Text t = new Text("hadoop");
    assertThat(t.getLength(), is(6));
    assertThat(t.getBytes().length, is(6));

    assertThat(t.charAt(2), is((int) 'd)); // char를 반환하는 String과 다름.
    assertThat("Out of bounds", t.charAt(100), is(-1));
    ```
    
  - find() 메서드를 사용하는 예제. (String의 indexOf()와 유사)
    ```
    Text t = new Text("hadoop");
    assertThat("Find a substring", t.find("do"), is(2));
    assertThat("Find first 'o'", t.find("o"), is(3));
    assertThat("Finds 'o' from position 4 or later", t.find("o", 4), is(4));
    assertThat("No match", t.find("pig"), is(-1));
    ```
    
- 유니코드.
  한 바이트 이상을 인코딩하는 문자를 사용하면 Text와 String의 차이점을 명확히 알 수 있다.

#### 유니코드 문자
| 유니코드 코드 포인트 | 이름                       | UTF-8 코드 단위 | 자바 표기   |
| -------------------- | ----------------------------|-----------------|------------- |
| U+0041               | 라틴 대문자 A               | 41              | \u0041       |
| U+00DF               | 세련된 라틴 소문자 S        | c3 9f           | \u00DF       |
| U+6771               | N/A (한의 통일된 표의문자)  | e6 9d b1        | \u6771       |
| U+10400              | 긴 데저렛 대문자 I          | f0 90 90 80     | \uD801\uDC00 |
- 마지막 문자는 보충 문자에 해당하고 **대행 쌍** 으로 알려진 2개의 자바 char로 표기.

- 반복.
  - 먼저 text 객체를 java.nio.ByteBuffer로 변환.
  - 이 버퍼로 text의 bytesToCodePoint() 정적 메서드를 반복해서 호출.
    - byteToCodePoint() : 다음 코드 포인트를 int 자료형으로 추출하고 버퍼의 위치를 갱신.
  
- 가변성.
  - String과의 차이점은 Text는 가변적이라는 것.
  - set() 메서드 중 하나를 호출하여 재사용할 수 있음.
    
- 스트링으로 변환.
  - Text는 풍부한 문자열 조작 API를 갖고 있지 않아서 많은 상황에서 Text 객체를 String으로 변환할 필요가 있음.
  - toString() 메서드를 이용해 Text 객체를 String으로 변환.

**BytesWritable**
바이너리 데이터의 배열에 대한 일종의 래퍼.

- 가변적이고 그 값은 set() 메서드를 호출하여 변경할 수 있음.
- getBytes() 메서드에서 반환된 바이트 배열의 크기는 실제 저장된 데이터 크기를 반영하지 않을 수 있음.
- getLength() 메서드를 호출하여 BytesWritable의 크기 결정 가능.


**NullWritable**
Writable의 특별한 유형 중 하나로 길이가 0인 직렬화를 가짐.
- 위치 표시자로 사용됨.

**ObjectWritable과 GenericWritable**
- ObjectWritable : 자바의 기본 자료형, String, enum, Writable, null 또는 이러한 자료형의 배열을 위한 범용 래퍼
  - 하둡 RPC에서 메서드 인자와 반환 타입을 집결하고 역집결하는 데 사용됨.
- GenericWritable : 자료형의 수가 적고 미리 알려진 경우에 정적 자료형 배열과 그 자료형에 대한 직렬화된 참조인 배열의 인덱스를 사용하여 공간 낭비를 줄일 수 있음. 


**Writable 컬렉션**
org.apache.hadoop.io 패키지에는 6개의 Writable 컬렉션 타입이 있음.
- ArrayWritable & TwoArrayWritable : Writable 인스턴스의 배열과 2차원 배열의 Writable 구현체.
- ArrayPrimitiveWritable : 자바 기본 배열에 대한 래퍼.
- MapWritable & SortedMapWritable : 각각 java.util.Map<Writable, Writable>과 java.util.SortedMap<WritableComparable, Writable>의 구현체.
- EnumSetWritable : 열거 자료형 집합을 위해 사용.

### 커스텀 Writable 구현하기
커스텀 Writable을 작성하면 바이너리 표현도 정렬 순서에 대한 완전한 통제가 가능.

- TextPair의 write() 메서드 : 각 Text 객체에 출력을 위임하는 방식으로 각 Text 객체를 출력 스트림에 순차적으로 직렬화함.
- readFields() 메서드 : 각 Text 객체에 입력을 위임하여 입력 스트림으로부터 바이트를 역직렬화함.
- compareTo() 메서드 : 사용자가 원하는 순서대로 문자열을 나열할 수 있음.

**성능 향상을 위해 RawComparator 구현하기**

두 TextPair 객체를 직렬화된 표현만 보고 비교할 수 있음. (TextPair는 두 Text 객체를 합친 것이므로)
- 초기의 길이를 읽고 첫 번째 Text 객체의 바이트 표현이 얼마나 긴지 알아내기.
- 이후 Text의 Rawcomparator에 위임.
- 두 번째 문자열에 대한 적절한 오프셋을 너겨주어 호출.

**커스텀 비교자**

커스텀 비교자도 RawComparator로 작성하는 것이 좋다.
- RawComparator : 기본 비교자에서 정의한 자연 정렬 순서와는 다른 정렬 순서를 구현한 비교자.


### 직렬화 프레임워크

하둡은 플러그인 직렬화 프레임워크 API를 제공함.



**직렬화 IDL**


## 파일 기반 데이터 구조

### SequenceFile

**SequenceFile 만들기**

**SequenceFile 읽기**

**명령행 인터페이스로 SequenceFile 출력하기**

**SequenceFile 정렬하고 병합하기**

**SequenceFile 포맷**
<img width="971" height="550" alt="image" src="https://github.com/user-attachments/assets/a95c2319-bd77-4276-be68-b7be042615d8" />


<img width="1034" height="438" alt="image" src="https://github.com/user-attachments/assets/4eb632ac-fd74-4854-9768-023cf933b3be" />


### MapFile

***MapFile의 변형**

### 기타 파일 포맷과 컬럼 기반 파일 포맷
<img width="511" height="487" alt="image" src="https://github.com/user-attachments/assets/a0e22dd7-0abb-41e0-863f-a6658714b3ba" />



