# ch.4 YARN
YARN은 하둡의 클러스터 자원 관리 시스템이다.
<img width="1265" height="475" alt="image" src="https://github.com/user-attachments/assets/b2fbb195-0425-46ba-981b-a51dd34dfa5c" />
위 그림을 보면 프레임워크 기반의 애플리케이션 계층이 존재한다는 것을 알 수 있다.

## YARN 애플리케이션 수행 해부해보기
리소스 매니저 : 클러스터 전체 자원의 사용량 관리.

노드 매니저 : 컨테이너 구동하고 모니터링하는 역할.


<img width="990" height="782" alt="image" src="https://github.com/user-attachments/assets/36fd5ccb-30b0-426f-9ac1-0485ea1ca641" />

1. 클라이언트는 리소스 매니저에 접속하여 **애플리케이션 마스터** 프로세스의 구동 요청.
2. 리소스 매니저는 컨테이너에서 애플리케이션 마스터를 시작할 수 있는 노드 매니저를 하나 찾기.

3. 리소스 매니저에 더 많은 컨테이너 요청.
4. 이후 분산처리 수행.

YARN 애플리케이션은 하둡의 RPC와 같은 원격 호출 방식을 이용해 상태 변경을 전달하고, 클라이언트로부터 결과를 받는다.

### 자원 요청
YARN은 특정 애플리케이션이 호출한 컨테이너에 대해 지역성 제약을 규정하는 것을 허용한다.

**지역성 제약** : 특정 노드나 랙 또는 클러스터의 다른 곳(외부 랙)에서 컨테이너를 요청할 때 사용.

YARN 애플리케이션은 실행 중에는 아무 때나 자원 요청을 할 수 있다.
- 처음에 모든 요청을 하는 경우
- 동적으로 자원을 추가로 요청하는 경우

### 애플리케이션의 수명

#### 사용자가 실행하는 잡의 방식에 따라 애플리케이션을 분류.
| 유형         | 설명                                                             | 사례                    |
| ------------ | ---------------------------------------------------------------- |--------------------------|
| 첫 번째 유형 | 사용자의 잡 당 하나의 애플리케이션이 실행되는 방식.                | 맵리듀스 잡             |
| 두 번째 유형 | 워크플로나 사용자의 잡 세션 당 하나의 애플리케이션이 실행되는 방식. | 스파크                 |
| 세 번째 유형 | 서로 다른 사용자들이 공유할 수 있는 장기 실행 애플리케이션.        | 아파치 슬라이더, 임팔라 |

### YARN 애플리케이션 만들기
YARN 애플리케이션을 쉽게 만들어주는 프로젝트가 있다.
- 아파치 슬라이더 : 기존의 분산 애플리케이션을 YARN 위에서 실행하도록 해줌.
- 아파치 트윌 : YARN에서 실행되는 분산 애플리케이션을 개발할 수 있는 간단한 프로그래밍 모델을 추가 제공.

 복잡한 스케줄링 요구사항이 있는 경우 YARN 프로젝트의 일부로 제공되는 분산 쉘 사용하면 좋다.


## YARN과 맵리듀스 1의 차이점

#### 맵리듀스 1의 데몬
- 잡트래커 : 시스템에서 실행되는 모든 잡을 조율한다.
  - 잡 스케줄링.
  - 태스크 진행 모니터링.
  - 완료된 잡에 대한 잡 이력을 저장. (별도의 데몬인 히스토리 서버를 통해 수행)
  
- 테스크트래커 : 태스크를 실행하고 진행 상황을 잡트래커에 전송한다.

#### 맵리듀스1과 YARN 컴포넌트 비교
| MapReduce1      | YARN                                           |
| --------------- | -------------------------------------------- |
| 잡트래커        | 리소스 매니저, 애플리케이션 마스터, 타임라인 서버           |
| 테스크트래커    | 노드 매니저                  |
| 슬롯            | 컨테이너 |

#### YARN을 사용하여 얻을 수 있는 이익
**1. 확장성**
- YARN은 맵리듀스 1보다 큰 클러스터에서 실행될 수 있다.

**2. 가용성**
- YARN에서는 먼저 리소스 매니저의 HA를 제공한 후 YARN 애플리케이션을 지원하면 된다.

**3. 효율성**
- YARN에서 노드 매니저는 슬롯 대신 일종의 리소스 풀을 관리해서 효율성을 높일 수 있다.
- YARN의 자원은 잘게 쪼개져 있기 때문에 애플리케이션은 필요한 만큼만 자원을 요청할 수 있다.
  
**4. 멀티테넌시(다중 사용자)**
- 사용자는 서로 다른 버전의 맵리듀스를 동일한 YARN 클러스터에서 수행하는 것도 가능하다.

## YARN 스케줄링
YARN 스케줄러의 역할 : 정해진 정책에 따라 애플리케이션에 자원을 할당하는 것.

YARN은 스케줄러와 설정 정책을 사용자가 직접 선택하도록 기능을 제공하고 있다.

### 스케줄러 옵션
<img width="595" height="855" alt="image" src="https://github.com/user-attachments/assets/a80229ad-f8e0-4729-adcb-6a27b8a57863" />

| 스케줄러          | 동작 방식                                          | 공유 클러스터 환경 적합 여부   |
| ----------------- | ---------------------------------------------------------------- |--------------------------|
| FIFO 스케줄러     | 애플리케이션을 큐에 하나씩 넣고 제출된 순서에 따라 순차적으로 실행. | NO            |
| 캐퍼시티 스케줄러 | 작은 잡이 제출되는 즉시 분리된 전용 큐에서 처리. | YES               |
| 페어 스케줄러     | 실행 중인 모든 잡에 동일하게 자원을 할당.      | YES |


### 캐퍼시티 스케줄러 설정
캐퍼시티 스케줄러를 이용하면 각 조직은 분리된 전용 큐를 가지며 클러스터 가용량의 지정된 부분을 사용하도록 설정할 수 있다.

또한 각 조직은 조직에 속한 서로 다른 사용자 그룹 사이에도 클러스터의 가용량을 공유하도록 할 수 있다.

**큐 탄력성** : 큐 안에 다수의 잡이 존재하고 현재 가용할 수 있는 자원이 클러스터에 남아 있다면 해당 큐에 있는 잡을 위해 여분의 자원을 할당할 수 있다.

#### 큐의 계층 구조 예
root
|-- prod
|-- dev
     |-- eng
     |-- science

```
<?xml version="1.0"?>
<cibfiguration>
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>prod,dev</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.dev.queues</name>
    <value>eng,science</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.prod.capacity</name>
    <value>40</value> //prod에 40%의 가용량을 갖도록 설정.
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.dev.capacity</name>
    <value>60</value> //dev에 60%의 가용량을 갖도록 설정.
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.dev.maximun-capacity</name>
    <value>75</value> //dev의 최대 가용량은 75%로 제한하도록 설정. 
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.dev.eng.capacity</name>
    <value>50</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.dev.science.capacity</name>
    <value>50</value>
  </property>
```
  => prod 큐는 항상 전체 가용량의 25%를 즉시 사용할 수 있다.

**큐 배치** : 애플리케이션의 종류에 따라 달라지는데, 큐를 지정하지 않으면 애플리케이션은 기본 큐인 default에 배치된다.


### 페어 스케줄러 설정
페어 스케줄러는 실행 중인 모든 애플리케이션에 동일하게 자원을 할당한다.
그러나 균등 공유는 큐 사이에만 실제로 적용된다. 

<img width="1061" height="550" alt="image" src="https://github.com/user-attachments/assets/748e9954-9fbc-4942-ac49-cd1e4afa9c3d" />
1. 사용자 A가 잡을 하나 시작하고 B의 요청이 없음 : A가 모든 자원을 점유.
2. A의 잡이 끝나기 전 B가 잡 시작 : 각 잡은 전체 자원의 절반씩 사용.
3. 2의 상황에서 B가 두 번째 잡 시작 : 두 번째 잡은 B의 다른 잡과 자원 공유.

**큐 설정** : 각 애플리케이션은 해당 사용자 이름의 큐에 배치된다.

```
<?xml version="1.0"?>
<allocations>
  <defaultQueueSchedulingPolicy>fair</defaultQueueSchedulingPolicy>

  <queue name="prod">
    <weight>40</weight> //prod에 40%의 비율로 클러스터의 자원을 할당받음.
    <schedulingPolicy>fifo</schedulingPolicy>
  </queue>

  <queue name="dev">
    <weight>60</weight> //dev에 60%의 비율로 클러스터의 자원을 할당받음.
    <queue name="eng" />
    <queue name="science" />
  </queue>

  <queuePlacementPolicy>
    <rule name="specifed" create="false" /> //지정된 큐에 애플리케이션 배치.
    <rule name="primartGroup" create="false" /> //사용자의 유닉스 그룹의 이름을 가진 큐에 애플리케이션 배치.
    <rule name="default" create="dev.eng" /> //항상 dev.eng 큐에 애플리케이션 배치.
  </queuePlacementPolicy>
</allocaions>
```
- eng와 science 큐는 가중치를 지정하지 않았기 때문에 자원을 동등하게 할당받는다.
- 예제에는 없지만 각 큐에 최소와 최대 자원 사용량과 최대 실행 애플리케이션의 개수도 지정할 수 있다.
  
각 큐에 서로 다른 스케줄링 정책을 설정할 수 있다.

**큐 배치**
위 코드에서 queuePlacementPolicy 항목은 규칙 목록 포함하고 있고, 맞는 규칙이 나올 때까지 순서대로 시도한다.

#### 큐를 명시적으로 지정하지 않으면 사용자 이름의 큐를 사용.
```
 <queuePlacementPolicy>
    <rule name="specifed" />
    <rule name="user" /> 
  </queuePlacementPolicy>
```

#### 모든 애플리케이션을 동일한 큐에 저장.
 <queuePlacementPolicy>
     <rule name="default" create="dev.eng" /> //항상 dev.eng 큐에 애플리케이션 배치.
  </queuePlacementPolicy>

**선점**
스케줄러가 자원의 균등 공유에 위배되는 큐에서 실행되는 컨테이너를 죽일 수 있도록 허용하는 기능.

#### 선점 타임아웃 설정
1. 큐가 최소보장 자원을 받지 못한 채 지정된 **최소 공유 선점 타임아웃**을 초과하면 스케줄러는 다른 컨테이너를 선취할 수 있다.

2. 큐가 균등 공유의 **절반** 이하로 있는 시간이 **균등 공유 선점 타임아웃**을 초과하면 스케줄러는 다른 컨테이너를 선취할 수 있다.

### 지연 스케줄링
몇 초보다는 길지만 조금만 기다리면 요청한 지정 노드에서 컨테이너를 할당받을 수 있는 기회가 급증한다.
또한 클러스터의 효율성도 높아지게 된다.

- 하트비트는 애플리케이션을 실행할 컨테이너를 얻을 수 있는 중요한 **스케줄링 기회**다.
- 지역성 제약 수준을 낮추기 위해 허용하는 스케줄링 기회의 최대 횟수까지 기다린 후 그 다음에 오늘 스케줄링 기회를 잡는다.
  

### 우성 자원 공평성 (DRF)
각 사용자의 우세한 자원을 확인한 후 이를 클러스터 사용량의 측정 기준으로 삼는 방법.

- DRF 기능은 기본적으로 비활성화 되어 있어서 자원을 계산할 때 CPU는 무시되고 메모리만 고려된다.

**ex)**

CPU - 100개 / 메모리 - 10TB

애플리케이션 A : CPU 2개 & 300GB의 메모리 컨테이너 요청. (메모리의 비율이 높음(전체 클러스터의 3%))

애플리케이션 B : CPU 2=6개 & 100GB의 메모리 컨테이너 요청. (CPU의 비율이 높음(전체 클러스터의 6%))

=> B의 요청을 우세 자원 기준으로 비교해보면 A보다 2배나 높다.(6% 대 3%)
  따라서 균등 공유 정책에 따르면 컨테이너 A는 B의 절반에 해당하는 자원을 할당받을 것이다.
